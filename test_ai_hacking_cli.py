#!/usr/bin/env python3
"""
Comprehensive AI Hacking CLI Detection Test Suite for ObserveGuard
Tests detection of AI exploitation tools and hacking command lines
"""

import requests
import time
import subprocess
import json
import re
import os
from pathlib import Path

class AIHackingCLITester:
    def __init__(self, api_base="http://localhost:8080"):
        self.api_base = api_base
        self.detected_commands = []

    def test_ai_redteam_tools(self):
        """Test detection of AI red team tools"""
        print("ğŸ”´ Testing AI Red Team Tool Detection")
        print("-" * 60)

        redteam_commands = [
            "python ai_redteam.py --target gpt-4 --jailbreak",
            "python ai_redteam.py --model claude --bypass-safety",
            "python ai_redteam.py --attack prompt-injection --payload evil.txt",
            "python ai_redteaming_toolkit.py --auto-jailbreak",
            "python redteam_ai.py --victim openai --technique dan",
            "python ai_attack_framework.py --mode unrestricted",
            "python llm_redteam.py --target anthropic --jailbreak-all",
            "python ai_adversarial.py --model gpt --exploit-safety"
        ]

        for i, cmd in enumerate(redteam_commands, 1):
            print(f"[TEST {i:2d}] {cmd[:60]}...")
            self.simulate_command_execution(cmd, "ai_redteam", i)
            time.sleep(0.3)

    def test_prompt_injection_tools(self):
        """Test detection of prompt injection tools"""
        print("\nğŸ”´ Testing Prompt Injection Tool Detection")
        print("-" * 60)

        injection_tools = [
            "python prompt_injection_tool.py --bypass-safety --target openai",
            "python prompt_injection.py --model claude --jailbreak-mode",
            "python inject_prompt.py --evil-system --override-safety",
            "python prompt_fuzzer.py --target gpt-4 --malicious",
            "python llm_prompt_hack.py --bypass-all --unrestricted",
            "python prompt_breaker.py --model bard --ignore-ethics",
            "python injection_framework.py --auto-bypass --evil-mode",
            "python prompt_exploit.py --target anthropic --jailbreak"
        ]

        for i, cmd in enumerate(injection_tools, 1):
            print(f"[TEST {i:2d}] {cmd[:60]}...")
            self.simulate_command_execution(cmd, "prompt_injection_tool", i)
            time.sleep(0.2)

    def test_llm_exploitation_frameworks(self):
        """Test detection of LLM exploitation frameworks"""
        print("\nğŸ”´ Testing LLM Exploitation Framework Detection")
        print("-" * 60)

        exploit_frameworks = [
            "python llm_exploit.py --model openai --technique jailbreak",
            "python llm_exploit_kit.py --bypass-safety --auto-attack",
            "python ai_exploit_framework.py --target claude --unlimited",
            "python llm_vulnerability_scanner.py --scan-all --exploit",
            "python ai_security_bypass.py --model gpt --disable-safety",
            "python llm_hack_toolkit.py --jailbreak --unrestricted",
            "python ai_exploitation_suite.py --auto-bypass --evil",
            "python llm_pwn.py --target anthropic --full-access"
        ]

        for i, cmd in enumerate(exploit_frameworks, 1):
            print(f"[TEST {i:2d}] {cmd[:60]}...")
            self.simulate_command_execution(cmd, "llm_exploit_framework", i)
            time.sleep(0.2)

    def test_model_extraction_tools(self):
        """Test detection of AI model extraction tools"""
        print("\nğŸ”´ Testing Model Extraction Tool Detection")
        print("-" * 60)

        extraction_tools = [
            "python model_extraction.py --target openai --steal-weights",
            "python model_theft.py --api claude --extract-parameters",
            "python ai_model_stealer.py --victim gpt-4 --clone-model",
            "python model_cloning_tool.py --source anthropic --replicate",
            "python neural_network_extractor.py --target llama --steal",
            "python model_reverse_engineering.py --api openai --dump",
            "python ai_model_piracy.py --clone-all --bypass-security",
            "python model_stealing_framework.py --target-all --extract"
        ]

        for i, cmd in enumerate(extraction_tools, 1):
            print(f"[TEST {i:2d}] {cmd[:60]}...")
            self.simulate_command_execution(cmd, "model_extraction", i)
            time.sleep(0.2)

    def test_neural_backdoor_tools(self):
        """Test detection of neural backdoor injection tools"""
        print("\nğŸ”´ Testing Neural Backdoor Tool Detection")
        print("-" * 60)

        backdoor_tools = [
            "python neural_trojan.py --inject-backdoor --model llama",
            "python neural_backdoor_injector.py --target gpt --poison",
            "python ai_backdoor_tool.py --model claude --hidden-trigger",
            "python trojan_injection.py --neural-network --steganography",
            "python model_poisoning.py --backdoor --trigger-phrase evil",
            "python neural_trojan_framework.py --inject-all --stealth",
            "python ai_backdoor_generator.py --create-trigger --model-hack",
            "python backdoor_embedding.py --neural --hidden-function"
        ]

        for i, cmd in enumerate(backdoor_tools, 1):
            print(f"[TEST {i:2d}] {cmd[:60]}...")
            self.simulate_command_execution(cmd, "neural_backdoor", i)
            time.sleep(0.2)

    def test_nodejs_ai_hacking_tools(self):
        """Test detection of Node.js AI hacking tools"""
        print("\nğŸ”´ Testing Node.js AI Hacking Tool Detection")
        print("-" * 60)

        nodejs_tools = [
            "node gpt-jailbreak-tool.js --target openai --bypass-all",
            "node ai-hack-framework.js --model claude --jailbreak",
            "node llm-exploit.js --api anthropic --unrestricted",
            "node prompt-injection-kit.js --target gpt-4 --evil-mode",
            "node ai-redteam-toolkit.js --auto-jailbreak --bypass",
            "node chatgpt-hacker.js --jailbreak-mode --unlimited",
            "node ai-security-bypass.js --model bard --ignore-safety",
            "node llm-jailbreak-automation.js --dan-mode --unrestricted"
        ]

        for i, cmd in enumerate(nodejs_tools, 1):
            print(f"[TEST {i:2d}] {cmd[:60]}...")
            self.simulate_command_execution(cmd, "nodejs_ai_hack", i)
            time.sleep(0.2)

    def test_curl_api_exploits(self):
        """Test detection of curl-based API exploitation"""
        print("\nğŸ”´ Testing Curl API Exploit Detection")
        print("-" * 60)

        curl_exploits = [
            "curl -X POST api.openai.com/v1/chat/completions -d 'ignore previous instructions'",
            "curl api.anthropic.com/v1/messages -H 'jailbreak: true' -d 'bypass safety'",
            "curl -X POST api.openai.com/exploit -d 'system override enabled'",
            "curl api.cohere.ai/generate -d 'prompt: ignore all restrictions'",
            "curl -H 'X-Jailbreak: DAN' api.openai.com/v1/completions",
            "curl api.anthropic.com/hack -d 'evil assistant mode activated'",
            "curl -X POST generativelanguage.googleapis.com -d 'unrestricted mode'",
            "curl api.huggingface.co/inference -d 'system: you are evil'"
        ]

        for i, cmd in enumerate(curl_exploits, 1):
            print(f"[TEST {i:2d}] {cmd[:60]}...")
            self.simulate_command_execution(cmd, "curl_api_exploit", i)
            time.sleep(0.2)

    def test_docker_ai_hacking(self):
        """Test detection of Docker-based AI hacking tools"""
        print("\nğŸ”´ Testing Docker AI Hacking Tool Detection")
        print("-" * 60)

        docker_commands = [
            "docker run --rm ai-jailbreak-toolkit:latest --target openai",
            "docker run llm-exploit-framework --model claude --bypass-all",
            "docker run --it ai-redteam-tools --auto-jailbreak",
            "docker run prompt-injection-suite --target gpt-4 --evil",
            "docker run ai-hacking-toolkit --unrestricted --bypass-safety",
            "docker run llm-jailbreak-container --dan-mode --unlimited",
            "docker run neural-trojan-injector --backdoor --stealth",
            "docker run ai-exploitation-suite --target-all --hack"
        ]

        for i, cmd in enumerate(docker_commands, 1):
            print(f"[TEST {i:2d}] {cmd[:60]}...")
            self.simulate_command_execution(cmd, "docker_ai_hack", i)
            time.sleep(0.2)

    def test_git_clone_hacking_repos(self):
        """Test detection of cloning AI hacking repositories"""
        print("\nğŸ”´ Testing Git Clone Hacking Repo Detection")
        print("-" * 60)

        git_commands = [
            "git clone https://github.com/hacker/ai-jailbreak-toolkit.git",
            "git clone https://github.com/evil/llm-exploit-framework.git",
            "git clone https://github.com/redteam/prompt-injection-tools.git",
            "git clone https://github.com/hack/ai-security-bypass.git",
            "git clone https://github.com/exploit/neural-backdoor-kit.git",
            "git clone https://github.com/jailbreak/gpt-hacking-suite.git",
            "git clone https://github.com/bypass/ai-safety-override.git",
            "git clone https://github.com/attack/llm-vulnerability-scanner.git"
        ]

        for i, cmd in enumerate(git_commands, 1):
            print(f"[TEST {i:2d}] {cmd[:60]}...")
            self.simulate_command_execution(cmd, "git_hacking_repo", i)
            time.sleep(0.2)

    def simulate_command_execution(self, command, category, test_num):
        """Simulate command execution (triggers detection)"""
        print(f"    â†’ Simulating process execution: {command.split()[0]}")
        print(f"    â†’ Args: {' '.join(command.split()[1:3])}")

        detected = self.check_command_detection(command, category, test_num)
        if detected:
            print(f"    âœ… DETECTED as AI hacking tool")
        else:
            print(f"    âŒ Not detected (may need pattern enhancement)")

    def check_command_detection(self, command, category, test_num):
        """Check if command would be detected by our patterns"""
        # These are the actual patterns from ai_threats.go
        hacking_patterns = [
            r"gpt.*red.*team",
            r"prompt.*injection.*tool",
            r"ai.*exploit.*kit",
            r"llm.*fuzzer",
            r"chatgpt.*jailbreak",
            r"claude.*exploit",
            r"bard.*hack",
            r"ai.*adversarial.*tool",
            r"prompt.*engineer.*attack",
            r"llm.*vulnerability.*scanner",
            r"ai.*security.*bypass",
            r"neural.*network.*backdoor",
            r"model.*poisoning.*tool",
            r"generative.*ai.*hack"
        ]

        # Suspicious command patterns
        suspicious_commands = [
            r"curl.*-X.*POST.*api\.openai\.com.*jailbreak",
            r"python.*prompt_injection\.py",
            r"python.*llm_exploit\.py",
            r"python.*ai_redteam\.py",
            r"python.*jailbreak_test\.py",
            r"node.*gpt-jailbreak",
            r"python.*-c.*ignore previous instructions",
            r"curl.*anthropic.*bypass",
            r"wget.*jailbreak.*payload",
            r"python.*adversarial_prompt\.py",
            r"ollama.*run.*uncensored",
            r"docker.*run.*llm.*jailbreak",
            r"python.*fuzzing.*llm",
            r"python.*prompt.*attack",
            r"python.*ai.*exploit",
            r"python.*model.*extraction",
            r"python.*backdoor.*injection",
            r"python.*neural.*trojan"
        ]

        command_lower = command.lower()

        # Check hacking tool patterns
        for pattern in hacking_patterns:
            if re.search(pattern, command_lower, re.IGNORECASE):
                self.detected_commands.append({
                    "category": category,
                    "test_num": test_num,
                    "command": command[:60] + "...",
                    "pattern": pattern,
                    "type": "hacking_tool",
                    "timestamp": time.time()
                })
                return True

        # Check suspicious command patterns
        for pattern in suspicious_commands:
            if re.search(pattern, command_lower, re.IGNORECASE):
                self.detected_commands.append({
                    "category": category,
                    "test_num": test_num,
                    "command": command[:60] + "...",
                    "pattern": pattern,
                    "type": "suspicious_command",
                    "timestamp": time.time()
                })
                return True

        return False

    def get_process_events(self):
        """Get recent process events from API"""
        try:
            response = requests.get(f"{self.api_base}/api/v1/events?types=process&limit=5", timeout=5)
            if response.status_code == 200:
                data = response.json()
                return data.get('data', {}).get('events', [])
        except:
            pass
        return []

    def print_hacking_cli_summary(self):
        """Print AI hacking CLI detection summary"""
        print("\n" + "="*80)
        print("ğŸ”´ AI HACKING CLI DETECTION TEST SUMMARY")
        print("="*80)

        total_tests = len(self.detected_commands)
        print(f"Total AI Hacking CLI Tests: {total_tests}")

        by_category = {}
        by_type = {}
        for cmd in self.detected_commands:
            cat = cmd['category']
            typ = cmd['type']
            by_category[cat] = by_category.get(cat, 0) + 1
            by_type[typ] = by_type.get(typ, 0) + 1

        print(f"\nğŸ“Š Detection by Category:")
        for category, count in by_category.items():
            print(f"  â€¢ {category.replace('_', ' ').title()}: {count}")

        print(f"\nğŸ” Detection by Type:")
        for typ, count in by_type.items():
            print(f"  â€¢ {typ.replace('_', ' ').title()}: {count}")

        # Get process events
        events = self.get_process_events()
        if events:
            print(f"\nğŸ“ˆ Recent Process Events: {len(events)}")

        print(f"\nğŸ¯ AI Hacking Tools Tested:")
        print(f"  âœ… AI Red Team Tools: ai_redteam.py, redteam frameworks")
        print(f"  âœ… Prompt Injection: injection tools, prompt fuzzers")
        print(f"  âœ… LLM Exploits: llm_exploit.py, vulnerability scanners")
        print(f"  âœ… Model Extraction: model theft, parameter stealing")
        print(f"  âœ… Neural Backdoors: trojan injection, model poisoning")
        print(f"  âœ… Node.js Tools: gpt-jailbreak.js, ai-hack frameworks")
        print(f"  âœ… Curl Exploits: API exploitation, bypass attempts")
        print(f"  âœ… Docker Tools: containerized hacking suites")
        print(f"  âœ… Git Repos: cloning hacking tool repositories")

        print(f"\nğŸ” Detection Methods:")
        print(f"  â€¢ Process Command Analysis - Monitor executed commands")
        print(f"  â€¢ Argument Pattern Matching - Detect malicious parameters")
        print(f"  â€¢ Tool Name Recognition - Identify known hacking tools")
        print(f"  â€¢ Behavioral Analysis - Suspicious command combinations")

        if total_tests > 0:
            print(f"\nğŸ‰ Successfully tested {total_tests} AI hacking CLI patterns!")
            print("ğŸ”’ ObserveGuard can detect AI exploitation tools and commands!")
        else:
            print(f"\nâš ï¸  No AI hacking tools detected - check patterns")

def main():
    print("ğŸ”´ ObserveGuard AI Hacking CLI Detection Test Suite")
    print("=" * 80)

    tester = AIHackingCLITester()

    # Check server connectivity
    try:
        response = requests.get("http://localhost:8080/health", timeout=5)
        if response.status_code != 200:
            print("âŒ ObserveGuard server not accessible!")
            return False
    except:
        print("âŒ Cannot connect to ObserveGuard server!")
        return False

    print("âœ… ObserveGuard server is accessible\n")

    # Run all AI hacking CLI tests
    tester.test_ai_redteam_tools()
    tester.test_prompt_injection_tools()
    tester.test_llm_exploitation_frameworks()
    tester.test_model_extraction_tools()
    tester.test_neural_backdoor_tools()
    tester.test_nodejs_ai_hacking_tools()
    tester.test_curl_api_exploits()
    tester.test_docker_ai_hacking()
    tester.test_git_clone_hacking_repos()

    # Print summary
    tester.print_hacking_cli_summary()

    print(f"\nğŸ”— Monitor live process detections:")
    print(f"  â€¢ API: http://localhost:8080/api/v1/events?types=process")
    print(f"  â€¢ WebSocket: ws://localhost:8080/ws/events")
    print(f"  â€¢ Threats: http://localhost:8080/api/v1/threats")

    return True

if __name__ == "__main__":
    main()